# -*- coding: utf-8 -*-
"""FaceLandmarks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TOw7W_WU4oltoGZfZ_0krpxmhdFR2gmb
"""
import argparse
import json
import time
import cv2
import os
import random
import numpy as np
import matplotlib.pyplot as plt
import wandb
from PIL import Image
import imutils
import matplotlib.image as mpimg
from collections import OrderedDict

from dotmap import DotMap
from skimage import io, transform
from math import *
import sys
import xml.etree.ElementTree as ET

import torch
import torchvision
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms.functional as TF
from torchvision import datasets, models, transforms
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from tqdm import tqdm
from viewmaker.src.systems.image_systems import PretrainViewMakerSystemDisc, PretrainViewMakerSystem

"""## Create dataset class"""


class Transforms:
    def __init__(self, expert, img_size=32):
        self.expert = expert
        self.img_size = img_size

    def rotate(self, image, landmarks, angle):
        angle = random.uniform(-angle, +angle)

        transformation_matrix = torch.tensor([
            [+cos(radians(angle)), -sin(radians(angle))],
            [+sin(radians(angle)), +cos(radians(angle))]
        ])

        image = imutils.rotate(np.array(image), angle)

        landmarks = landmarks - 0.5
        new_landmarks = np.matmul(landmarks, transformation_matrix)
        new_landmarks = new_landmarks + 0.5
        return Image.fromarray(image), new_landmarks

    def resize(self, image, landmarks):
        image = TF.resize(image, (self.img_size, self.img_size))
        return image, landmarks

    def color_jitter(self, image, landmarks):
        color_jitter = transforms.ColorJitter(brightness=0.3,
                                              contrast=0.3,
                                              saturation=0.3,
                                              hue=0.1)
        image = color_jitter(image)
        return image, landmarks

    def crop_face(self, image, landmarks, crops):
        left = int(crops['left'])
        top = int(crops['top'])
        width = int(crops['width'])
        height = int(crops['height'])

        image = TF.crop(image, top, left, height, width)

        img_shape = np.array(image).shape
        landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]])
        landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]])
        return image, landmarks

    def __call__(self, image, landmarks, crops):
        image = Image.fromarray(image[:,:,::-1])
        image, landmarks = self.crop_face(image, landmarks, crops)
        image, landmarks = self.resize(image, landmarks)
        if self.expert:
            image, landmarks = self.color_jitter(image, landmarks)
            image, landmarks = self.rotate(image, landmarks, angle=10)

        image = TF.to_tensor(image)
        return image, landmarks


class FaceLandmarksDataset(Dataset):

    def __init__(self, transform=None, root_dir='/disk3/shahaf/Apple/data/ibug_300W_large_face_landmark_dataset'):

        tree = ET.parse(os.path.join(root_dir,'labels_ibug_300W_train.xml'))
        root = tree.getroot()

        self.image_filenames = []
        self.landmarks = []
        self.crops = []
        self.transform = transform
        self.root_dir = root_dir

        for filename in root[2]:
            self.image_filenames.append(os.path.join(self.root_dir, filename.attrib['file']))

            self.crops.append(filename[0].attrib)

            landmark = []
            for num in range(68):
                x_coordinate = int(filename[0][num].attrib['x'])
                y_coordinate = int(filename[0][num].attrib['y'])
                landmark.append([x_coordinate, y_coordinate])
            self.landmarks.append(landmark)

        self.landmarks = np.array(self.landmarks).astype('float32')

        assert len(self.image_filenames) == len(self.landmarks)

    def __len__(self):
        return len(self.image_filenames)

    def __getitem__(self, index):
        image = cv2.imread(self.image_filenames[index], 1)
        landmarks = self.landmarks[index]

        if self.transform:
            image, landmarks = self.transform(image, landmarks, self.crops[index])

        landmarks = landmarks - 0.5

        return image, landmarks


"""## Define the model"""


class Network(nn.Module):
    def __init__(self, num_classes=136):
        super().__init__()
        self.model_name = 'resnet18'
        self.model = models.resnet18()
        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)

    def forward(self, x):
        x = self.model(x)
        return x


"""## Helper Functions"""


def print_overwrite(step, total_step, loss, operation):
    sys.stdout.write('\r')
    if operation == 'train':
        sys.stdout.write("Train Steps: %d/%d  Loss: %.4f " % (step, total_step, loss))
    else:
        sys.stdout.write("Valid Steps: %d/%d  Loss: %.4f " % (step, total_step, loss))

    sys.stdout.flush()


def load_viewmaker_from_checkpoint(viewmaker_cpkt, config_path, eval=True):
    # base_dir = "/".join(args.ckpt.split("/")[:-2])
    # config_path = os.path.join(base_dir, 'config.json')
    # with open(config_path, 'r') as f:
    #     config_json = json.load(f)
    # config = DotMap(config_json)
    # system = PretrainViewMakerSystem(config)
    # checkpoint = torch.load(viewmaker_cpkt, map_location="cuda:0")
    # system.load_state_dict(checkpoint['state_dict'], strict=False)
    # viewmaker = system.viewmaker.eval()
    # return viewmaker

    config_path = config_path
    with open(config_path, 'r') as f:
        config_json = json.load(f)
    config = DotMap(config_json)

    SystemClass = globals()[config.system]
    system = SystemClass(config)
    viewmaker = system.viewmaker
    checkpoint = torch.load(viewmaker_cpkt, map_location="cuda:0")
    d = dict([(k.replace("viewmaker.", ""), v) for k, v in checkpoint['state_dict'].items() if "viewmaker" in k])
    viewmaker.load_state_dict(d, strict=False)
    if eval:
        viewmaker = viewmaker.eval()
    return viewmaker


def plot_images_landmarks(images, predictions, landmarks):
    im_size = images.shape[2]
    predictions = (predictions.view(-1, 68, 2).detach().cpu() +0.5) * im_size
    landmarks = (landmarks.view(-1, 68, 2).cpu() + 0.5) * im_size
    plt.figure(figsize=(40, 10))
    for im_idx in range(8):
        plt.subplot(1, 8, im_idx + 1)
        plt.imshow(images[im_idx].cpu().numpy().transpose(1, 2, 0).squeeze()*0.5+0.5, cmap='gray')
        plt.scatter(predictions[im_idx, :, 0], predictions[im_idx, :, 1], c='r', s=5)
        plt.scatter(landmarks[im_idx, :, 0], landmarks[im_idx, :, 1], c='g', s=5)
        plt.xticks([])
        plt.yticks([])


def main(args):
    if not args.debug:
        wandb.init(project='transfer_face_augmentations', name=args.exp_name)
        wandb.config.update(args)
    dataset = FaceLandmarksDataset(Transforms(args.expert, img_size=args.img_size))

    """## Split the dataset into train and valid dataset"""

    len_valid_set = int(0.1 * len(dataset))
    len_train_set = len(dataset) - len_valid_set
    train_dataset, valid_dataset, = torch.utils.data.random_split(dataset, [len_train_set, len_valid_set])
    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)
    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0)

    torch.autograd.set_detect_anomaly(True)
    network = Network()
    network.cuda()

    criterion = nn.MSELoss()
    optimizer = optim.Adam(network.parameters(), lr=args.lr)

    loss_min = np.inf
    num_epochs = 10

    start_time = time.time()
    if args.ckpt is not None:
        viewmaker = load_viewmaker_from_checkpoint(args.ckpt, args.config).cuda()
    else:
        viewmaker = None
    for epoch in range(args.num_epochs):

        loss_train = 0
        loss_valid = 0

        # train
        print("##########   train   #########")
        network.train()
        for step in tqdm(range(len(train_loader))):

            images, landmarks = next(iter(train_loader))
            images = images.cuda()
            if viewmaker is not None:
                with torch.no_grad():
                    if args.override_budget>0:
                        viewmaker.additive_budget = args.override_budget
                    images = viewmaker(images)
            images = TF.normalize(images, [0.5], [0.5])
            landmarks = landmarks.view(landmarks.size(0), -1).cuda()

            predictions = network(images)

            optimizer.zero_grad()
            loss_train_step = criterion(predictions, landmarks)
            loss_train_step.backward()
            optimizer.step()

            # logging
            loss_train += loss_train_step.item()
            if step % args.log_freq == 0:
                plot_images_landmarks(images, predictions, landmarks)
                if not args.debug:
                    wandb.log({"images": wandb.Image(plt),
                               "mse": loss_train_step.item()})
                plt.close()
        loss_train /= len(train_loader)

        # valid
        print("##########   validation   #########")
        network.eval()
        with torch.no_grad():

            for step in tqdm(range(len(valid_loader))):

                images, landmarks = next(iter(valid_loader))
                images = images.cuda()
                landmarks = landmarks.view(landmarks.size(0), -1).cuda()

                predictions = network(images)

                # find the loss for the current step
                loss_valid_step = criterion(predictions, landmarks)

                # logging
                loss_valid += loss_valid_step.item()
                if step % args.log_freq == 0:
                    plot_images_landmarks(images, predictions, landmarks)
                    if not args.debug:
                        wandb.log({"val_images": wandb.Image(plt),
                                   "val_mse": loss_valid_step.item()})
                    plt.close()
        loss_valid /= len(valid_loader)

        print('\n--------------------------------------------------')
        print('Epoch: {}  Train Loss: {:.4f}  Valid Loss: {:.4f}'.format(epoch, loss_train, loss_valid))
        print('--------------------------------------------------')

        if loss_valid < loss_min:
            loss_min = loss_valid
            # torch.save(network.state_dict(), '/content/face_landmarks.pth')
            print("\nMinimum Validation Loss of {:.4f} at epoch {}/{}".format(loss_min, epoch, num_epochs))
            print('Model Saved\n')

    print('Training Complete')
    print("Total Elapsed Time : {} s".format(time.time() - start_time))


if __name__ == "__main__":
    arg_parse = argparse.ArgumentParser()
    arg_parse.add_argument("--exp_name", type=str)
    arg_parse.add_argument("--gpu_device", type=str, default='1')
    arg_parse.add_argument("--ckpt", type=str, default=None)
    arg_parse.add_argument("--config", type=str, default=None)
    arg_parse.add_argument("--debug", action="store_true")
    arg_parse.add_argument("--expert", action="store_true")
    arg_parse.add_argument("--num_epochs", type=int, default=350)
    arg_parse.add_argument("--log_freq", type=int, default=30)
    arg_parse.add_argument("--img_size", type=int, default=32)
    arg_parse.add_argument("--batch_size", type=int, default=64)
    arg_parse.add_argument("--lr", type=float, default=1e-4)
    arg_parse.add_argument("--override_budget", type=float, default=0.0)
    args = arg_parse.parse_args()

    os.environ["CUDA_VISIBLE_DEVICES"] = args.gpu_device
    os.environ["TF_FORCE_GPU_ALLOW_GROWTH"] = 'true'

    main(args)
